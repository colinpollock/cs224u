{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SST Transformers #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AdamW, BertForSequenceClassification, BertTokenizer, WarmupLinearSchedule\n",
    "\n",
    "import sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST specific setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_HOME = os.path.join('data', 'trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_tree(tree):\n",
    "    sentence = ' '.join(tree.leaves())\n",
    "    replacements = [\n",
    "        (\" 's\", \"'s\"),\n",
    "        (' .', '.'),\n",
    "        (' ,', ','),\n",
    "        (\"`` \", \"'\"),\n",
    "        (\" ''\", \"'\"),\n",
    "        (\" 'm\", \"'m\"),\n",
    "        (\" 've\", \"'ve\"),\n",
    "        (\" 't\", \"'t\"),\n",
    "        (\" 're\", \"'re\")\n",
    "    ]\n",
    "    \n",
    "    for from_, to in replacements:\n",
    "        sentence = sentence.replace(from_, to)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### end SST specific setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the train and dev datasets\n",
    "def get_texts_and_labels(reader):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - reader: sst.dev_reader, sst.train_reader, or sst.test_reader\n",
    "    Returns a pair:\n",
    "    * texts: a list of strings \n",
    "    * labels: a list of strings\n",
    "    \"\"\"\n",
    "    d = sst.build_dataset(SST_HOME, reader, phi=lambda x: None, class_func=sst.ternary_class_func, vectorize=False)\n",
    "    texts = [text_from_tree(tree) for tree in d['raw_examples']]\n",
    "    labels = d['y']\n",
    "    return texts, labels\n",
    "\n",
    "def _bert_tokenize(text, max_length=128):\n",
    "    tokenized = bert_tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    token_ids = tokenized['input_ids']\n",
    "    special_tokens_mask = tokenized['token_type_ids']\n",
    "    token_type_ids = tokenized['token_type_ids']\n",
    "    while len(token_ids) < max_length:\n",
    "        token_ids.append(0)\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "_y_mapper = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "def label_stoi(label_string):\n",
    "    return _y_mapper[label_string]\n",
    "\n",
    "def label_itos(label_int):\n",
    "    for label_string, label_int_ in _y_mapper.items():\n",
    "        if label_int == label_int_:\n",
    "            return label_string\n",
    "    assert False\n",
    "\n",
    "\n",
    "def make_attention_masks(token_ids_list):\n",
    "    attention_masks = []\n",
    "    for token_ids in token_ids_list:\n",
    "        mask = [float(token_id > 0) for token_id in token_ids]\n",
    "        attention_masks.append(mask)\n",
    "    \n",
    "    return torch.tensor(attention_masks)\n",
    "    \n",
    "\n",
    "def build_data_loader(reader, batch_size):\n",
    "    texts, labels = get_texts_and_labels(reader)\n",
    "    \n",
    "    labels_vector = torch.tensor([label_stoi(label) for label in labels])\n",
    "    token_ids = torch.tensor([_bert_tokenize(text) for text in texts])\n",
    "    attention_masks = make_attention_masks(token_ids)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(token_ids, attention_masks, labels_vector)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_bert(model, train_loader, epochs=1):\n",
    "    lr = 1e-3\n",
    "    max_grad_norm = 1.0\n",
    "    num_training_steps = 1000\n",
    "    num_warmup_steps = 100\n",
    "    warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n",
    "\n",
    "    # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "    optimizer = AdamW(bert_classifier.parameters()) \n",
    "\n",
    "    # PyTorch scheduler\n",
    "#     scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch} of {epochs}')\n",
    "        \n",
    "        epoch_loss = 0\n",
    "\n",
    "        num_batches = len(train_loader)\n",
    "        for batch_number, batch in enumerate(train_loader):\n",
    "            batch_token_ids, batch_attention_mask, batch_labels = batch\n",
    "            batch_token_ids = batch_token_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            model.train()\n",
    "            loss, logits = model(batch_token_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
    "            optimizer.step()\n",
    "#             scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            if batch_number % 10 == 0:\n",
    "                print(f'    Batch {batch_number} of {num_batches}. Loss: {loss}')\n",
    "\n",
    "        print('Epoch loss:', epoch_loss)\n",
    "        \n",
    "transformer_model_name = 'bert-base-uncased'\n",
    "bert_classifier = BertForSequenceClassification.from_pretrained(transformer_model_name, num_labels=3).to(device)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(transformer_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_data_loader(sst.train_reader, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 11.17 GiB total capacity; 10.71 GiB already allocated; 8.81 MiB free; 139.52 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-8f18ffca7336>\u001b[0m in \u001b[0;36mtrain_bert\u001b[0;34m(model, train_loader, epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient clipping is not in AdamW anymore (so you can use amp without issue)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs224u/venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs224u/venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 11.17 GiB total capacity; 10.71 GiB already allocated; 8.81 MiB free; 139.52 MiB cached)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_bert(bert_classifier, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = build_data_loader(sst.dev_reader, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bert(bert_classifier, dev_loader):\n",
    "    bert_classifier.eval()\n",
    "    total = correct = 0\n",
    "    for batch in dev_loader:\n",
    "        batch_token_ids, batch_attention_mask, batch_labels = batch\n",
    "        batch_token_ids = batch_token_ids.to(device)\n",
    "        batch_attention_mask = batch_attention_mask.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, = bert_classifier(batch_token_ids, attention_mask=batch_attention_mask)\n",
    "        predictions = logits.argmax(axis=1)\n",
    "        is_correct = batch_labels == predictions\n",
    "        total += len(is_correct)\n",
    "        correct += sum(is_correct)\n",
    "        return batch_labels.cpu().numpy(), logits.cpu().numpy()\n",
    "    \n",
    "    print('{} of {} correct ({:.2f}%)'.format(correct, total, correct / total * 100))\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229 of 1101 correct (0.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(229, device='cuda:0'), 1101)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_bert(bert_classifier, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, logits = eval_bert(bert_classifier, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 1, 2, 1, 2, 1, 1, 0, 0, 1, 2, 2, 2, 0, 1, 0, 1, 2, 0, 2,\n",
       "       2, 0, 0, 1, 0, 2, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ],\n",
       "       [ 0.30612442,  0.36633143, -0.2562312 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: the outputs for every example are the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
